name: Validate & Generate Metadata

on:
  push:
    branches: [ main ]
    paths:
      - 'commands/**'
      - 'scripts/extract-playbook-metadata.py'
      - 'scripts/validate-extracted-metadata.py'
      - 'scripts/generate-quick-ref.py'
      - '.playbook-extraction-schema.yaml'
      - '.github/workflows/validate-metadata.yml'
  pull_request:
    branches: [ main ]

permissions:
  contents: read
  pull-requests: write

jobs:
  extract-metadata:
    name: Extract Metadata from Commands
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Extract metadata
        run: |
          python scripts/extract-playbook-metadata.py --verbose

      - name: Verify metadata was created
        run: |
          if [ ! -f ".playbook-metadata.json" ]; then
            echo "ERROR: Metadata file not created"
            exit 1
          fi

          # Verify it's valid JSON and contains commands
          python3 -c "
          import json
          with open('.playbook-metadata.json') as f:
            data = json.load(f)
          commands = data.get('commands', {})
          print(f'✓ Extracted {len(commands)} commands')
          if len(commands) < 40:
            print('WARNING: Fewer than 40 commands extracted')
          "

      - name: Upload metadata artifact
        uses: actions/upload-artifact@v3
        with:
          name: metadata
          path: .playbook-metadata.json
          retention-days: 1

  validate-metadata:
    name: Validate Metadata Quality
    needs: extract-metadata
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Download metadata
        uses: actions/download-artifact@v3
        with:
          name: metadata

      - name: Validate metadata
        run: |
          python scripts/validate-extracted-metadata.py --strict

      - name: Check confidence threshold
        run: |
          python3 << 'EOF'
          import json

          with open('.playbook-metadata.json') as f:
            data = json.load(f)

          report = data.get('extraction_report', {})
          avg_confidence = report.get('average_confidence', 0)
          errors = report.get('errors', [])

          print(f"\n=== Metadata Quality Report ===")
          print(f"Average Confidence: {avg_confidence:.1%}")
          print(f"Critical Errors: {len(errors)}")

          # Check confidence gate (0.80 minimum)
          if avg_confidence < 0.80:
            print(f"\n❌ FAIL: Confidence {avg_confidence:.1%} < 0.80 minimum")
            exit(1)
          else:
            print(f"✓ PASS: Confidence {avg_confidence:.1%} >= 0.80 minimum")

          # Check critical errors
          if errors:
            print(f"\n⚠️  WARNING: {len(errors)} critical errors found")
            for error in errors[:5]:
              print(f"  - {error}")
          else:
            print("✓ No critical errors")

          print("\n=== Quality Thresholds ===")
          warnings = report.get('warnings', [])
          print(f"Warnings: {len(warnings)}")
          print(f"Total commands: {len(data.get('commands', {}))}")

          EOF

      - name: Upload validation report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: validation-report
          path: .playbook-metadata.json
          retention-days: 1

  generate-quick-ref:
    name: Generate Quick Reference
    needs: validate-metadata
    if: success()
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Download validated metadata
        uses: actions/download-artifact@v3
        with:
          name: metadata

      - name: Generate quick reference
        run: |
          python scripts/generate-quick-ref.py --metadata .playbook-metadata.json

      - name: Verify quick reference was created
        run: |
          if [ ! -f ".playbook-quick-ref.md" ]; then
            echo "ERROR: Quick reference file not created"
            exit 1
          fi

          lines=$(wc -l < .playbook-quick-ref.md)
          echo "✓ Generated quick reference ($lines lines)"

      - name: Show what changed
        run: |
          if git diff --quiet .playbook-quick-ref.md 2>/dev/null; then
            echo "No changes to quick reference"
          else
            echo "Quick reference updated:"
            git diff --no-index /dev/null .playbook-quick-ref.md 2>/dev/null | head -20 || true
          fi

      - name: Upload quick reference
        uses: actions/upload-artifact@v3
        with:
          name: quick-reference
          path: .playbook-quick-ref.md
          retention-days: 1

  report-quality:
    name: Report Quality Metrics
    needs: validate-metadata
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download validation report
        uses: actions/download-artifact@v3
        with:
          name: validation-report

      - name: Generate quality report
        id: report
        run: |
          python3 << 'EOF'
          import json

          with open('.playbook-metadata.json') as f:
            data = json.load(f)

          commands = data.get('commands', {})
          report = data.get('extraction_report', {})

          avg_confidence = report.get('average_confidence', 0)
          errors = report.get('errors', [])
          warnings = report.get('warnings', [])

          # Generate summary
          summary = f"""
          ## Playbook Metadata Quality Report

          - **Commands Extracted**: {len(commands)}
          - **Average Confidence**: {avg_confidence:.1%}
          - **Critical Errors**: {len(errors)}
          - **Warnings**: {len(warnings)}

          ### Confidence Status
          """

          if avg_confidence >= 0.80:
            summary += f"\n✅ **PASS** - Confidence {avg_confidence:.1%} meets 0.80 minimum threshold"
          else:
            summary += f"\n❌ **FAIL** - Confidence {avg_confidence:.1%} below 0.80 minimum threshold"

          if errors:
            summary += f"\n\n### Critical Errors ({len(errors)})\n"
            for error in errors[:5]:
              summary += f"- {error}\n"
            if len(errors) > 5:
              summary += f"- ... and {len(errors) - 5} more\n"

          if warnings:
            summary += f"\n### Warnings ({len(warnings)})\n"
            for warning in warnings[:5]:
              summary += f"- {warning}\n"
            if len(warnings) > 5:
              summary += f"- ... and {len(warnings) - 5} more\n"

          # Check if we should fail
          fail_job = len(errors) > 0 or avg_confidence < 0.80

          print(summary)
          print(f"\nQUALITY_PASS={'false' if fail_job else 'true'}")

          EOF

      - name: Fail if critical issues
        run: |
          python3 << 'EOF'
          import json

          with open('.playbook-metadata.json') as f:
            data = json.load(f)

          report = data.get('extraction_report', {})
          avg_confidence = report.get('average_confidence', 0)
          errors = report.get('errors', [])

          if errors:
            print(f"❌ FAIL: {len(errors)} critical errors found")
            exit(1)

          if avg_confidence < 0.80:
            print(f"❌ FAIL: Confidence {avg_confidence:.1%} < 0.80")
            exit(1)

          print(f"✅ PASS: All quality gates passed")
          exit(0)

          EOF
        continue-on-error: true

      - name: Comment on PR with metrics
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const data = JSON.parse(fs.readFileSync('.playbook-metadata.json', 'utf8'));
            const report = data.extraction_report || {};
            const commands = data.commands || {};

            const avg_confidence = (report.average_confidence * 100).toFixed(1);
            const errors = (report.errors || []).length;
            const warnings = (report.warnings || []).length;

            // Only comment if there are warnings or errors
            if (errors > 0 || warnings > 0 || avg_confidence < 80) {
              let comment = `## ⚠️ Metadata Quality Alert\n\n`;
              comment += `- **Commands Extracted**: ${Object.keys(commands).length}\n`;
              comment += `- **Average Confidence**: ${avg_confidence}%\n`;

              if (errors > 0) {
                comment += `- **Critical Errors**: ❌ ${errors}\n`;
              }
              if (warnings > 0) {
                comment += `- **Warnings**: ⚠️ ${warnings}\n`;
              }

              if (avg_confidence < 80) {
                comment += `\n**Action Required**: Confidence below 80% threshold. Review extraction quality.`;
              }

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
